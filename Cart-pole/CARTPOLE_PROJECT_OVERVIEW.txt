================================================================================
                    CART-POLE REINFORCEMENT LEARNING PROJECT
================================================================================

PROJECT: Classic Control Problem with Q-Learning
ALGORITHM: Q-Learning (Temporal Difference, Model-Free)
VISUALIZATION: Pygame-based real-time simulation
GOAL: Balance a pole on a moving cart using force control

================================================================================
1. PROJECT STRUCTURE
================================================================================

Cart-pole/
├── README.md
└── src/
    ├── main.py         # Main simulation loop, visualization, training
    ├── cartpole.py     # CartPole environment and physics
    ├── QAgent.py       # Q-Learning agent implementation
    └── config.py       # Hyperparameters and constants

================================================================================
2. THE CART-POLE PROBLEM
================================================================================

DESCRIPTION:
    A pole is attached by an un-actuated joint to a cart moving on a 
    frictionless track. The goal is to balance the pole upright by applying
    forces to the cart (left or right).

STATE SPACE (4 dimensions):
    - x: Cart position on track [-2.4, 2.4]
    - θ (theta): Pole angle from vertical [-0.5, 0.5] radians (~±28.6°)
    - x_dot: Cart velocity [-2.0, 2.0]
    - θ_dot (theta_dot): Angular velocity of pole [-2.0, 2.0]

ACTION SPACE (11 discrete actions):
    Forces: [-10, -7, -5, -3, -1, 0, 1, 3, 5, 7, 10] Newtons
    - Negative: Push cart left
    - Positive: Push cart right
    - Finer control near zero for precise balancing

TERMINATION CONDITIONS:
    - Pole angle exceeds ±0.5 radians (~±28.6°)
    - Cart position exceeds ±2.4 units

SUCCESS METRIC:
    - Number of steps before termination
    - Higher is better (longer balance time)

================================================================================
3. CORE COMPONENTS
================================================================================

3.1 CONFIG.PY - Configuration and Hyperparameters
--------------------------------------------------
Physical Constants:
    GRAVITY = 9.81 m/s²
    INIT_ANGLE_MIN_MAX = 3° (random start angle)

State Discretization:
    X_BINS = 8          (cart position)
    THETA_BINS = 12     (pole angle)
    X_DOT_BINS = 8      (cart velocity)
    THETA_DOT_BINS = 12 (angular velocity)
    
    Total states: 8 × 12 × 8 × 12 = 9,216 discrete states
    Total state-action pairs: 9,216 × 11 = 101,376

Q-Learning Hyperparameters:
    ALPHA = 0.3          # Learning rate
    GAMMA = 0.95         # Discount factor
    EPSILON_START = 1.0  # Initial exploration rate
    EPSILON_MIN = 0.01   # Minimum exploration
    EPSILON_DECAY = 0.999 # Decay per episode

Visualization:
    WIDTH, HEIGHT = 800, 400 pixels
    SCALE = 100.0        # World to screen scaling
    dt = 0.02 seconds    # Simulation timestep (50 Hz)

3.2 CARTPOLE.PY - Environment and Physics
------------------------------------------
CartPole Class:

    Physics Model:
        - Second-order differential equations for cart and pole
        - Derived from Lagrangian mechanics
        - Considers:
            * Cart mass (1.0 kg)
            * Pole mass (0.1 kg)
            * Pole length (0.5 m)
            * Applied force
            * Gravitational effects

    Key Methods:
        __init__(cart_mass, pole_mass, pole_length, dt):
            Initialize physics parameters

        reset():
            Randomize initial pole angle [-3°, +3°]
            Set cart at center with zero velocities

        dynamics(u):
            Compute accelerations from current state and force
            Returns: [x_dot, theta_dot, x_ddot, theta_ddot]

        step(u):
            Integrate dynamics forward by dt using Euler method
            Returns: next_state, cart_position, pole_tip_position

        discretize_state():
            Convert continuous state to discrete bin indices
            Clips values to valid ranges
            Returns: (x_bin, theta_bin, x_dot_bin, theta_dot_bin)

        is_terminal():
            Check if pole angle or cart position out of bounds

        get_reward(z_next, terminated):
            - Terminal state: -200,000 (large penalty)
            - Otherwise:
                * Angle reward: 1.0 - (|θ|/0.5)²  [0 to 1]
                * Stability reward: 0.5 × (1 - min(|θ_dot|/2.0, 1))
                * Center penalty: -0.1 × (|x|/2.4)²
            
            Encourages:
                ✓ Small angle (upright pole)
                ✓ Low angular velocity (stability)
                ✓ Staying near center

3.3 QAGENT.PY - Q-Learning Agent
---------------------------------
QLearningAgent Class:

    Q-Table Structure:
        5D numpy array: [x_bins, θ_bins, x_dot_bins, θ_dot_bins, n_actions]
        Shape: (8, 12, 8, 12, 11) = 101,376 entries
        Initialized with small random values [-0.01, 0.01] for exploration

    Methods:
        get_action(state, training=True):
            Epsilon-greedy policy:
            - With probability ε: random action (exploration)
            - With probability 1-ε: argmax_a Q(s,a) (exploitation)

        update(state, action, reward, next_state, terminated):
            Q-learning update rule:
            
            If terminal:
                Q(s,a) ← Q(s,a) + α × [R - Q(s,a)]
            
            Otherwise:
                Q(s,a) ← Q(s,a) + α × [R + γ × max_a' Q(s',a') - Q(s,a)]
            
            Where:
                α = learning rate (0.3)
                γ = discount factor (0.95)
                R = immediate reward
                s' = next state

        decay_epsilon():
            ε ← max(ε_min, ε × decay_rate)
            Gradually reduces exploration over time

3.4 MAIN.PY - Simulation and Training Loop
-------------------------------------------
Main Functions:
    world_to_screen(x, y):
        Converts physics coordinates to pixel coordinates

Training Loop:
    1. Get current discrete state
    2. Agent selects action (ε-greedy)
    3. Environment executes action
    4. Compute reward
    5. Agent updates Q-table (if training)
    6. Check termination
    7. Decay epsilon after each episode
    8. Render visualization

Interactive Controls:
    T: Toggle training mode ON/OFF
    S: Save Q-table to 'q_table.pkl'
    L: Load Q-table from 'q_table.pkl'
    F: Fast mode (skip rendering during training)
    R: Reset agent (clear Q-table, restart from scratch)

Visualization:
    - Cart: Black rectangle
    - Pole: Red line with blue tip
    - Track: Gray horizontal line
    - Real-time metrics:
        * Current episode number
        * Steps in current episode
        * Best episode steps achieved
        * Average over last 100 episodes
        * Current epsilon value
        * Pole angle in degrees
        * Applied force

Performance Tracking:
    - Episode counter
    - Best step count
    - Rolling average (last 100 episodes)
    - Epsilon decay monitoring

================================================================================
4. Q-LEARNING ALGORITHM EXPLAINED
================================================================================

4.1 Overview
------------
Q-Learning is a model-free, off-policy TD (Temporal Difference) algorithm.

Model-free: Doesn't require knowing environment dynamics
Off-policy: Learns optimal policy while following ε-greedy exploration

4.2 Q-Value Meaning
-------------------
Q(s, a) = Expected cumulative discounted reward starting from state s,
          taking action a, then following optimal policy

Goal: Learn Q*(s,a) = optimal action-value function

4.3 Update Rule
---------------
Q(s,a) ← Q(s,a) + α × [TD_error]

where TD_error = [R + γ × max_a' Q(s',a')] - Q(s,a)
                  
Components:
    - R: Immediate reward
    - γ × max Q(s',a'): Discounted best future value
    - α: Learning rate (step size)

4.4 Exploration vs Exploitation
--------------------------------
ε-greedy policy:
    - Early training (ε ≈ 1.0): Mostly random (explore)
    - Late training (ε → 0.01): Mostly greedy (exploit)

Exploration discovers new strategies
Exploitation uses known good strategies

4.5 Convergence
---------------
Under conditions:
    - All state-action pairs visited infinitely often
    - Learning rate decays appropriately
    - Bounded rewards

Q-Learning converges to Q* (optimal values)

================================================================================
5. IMPLEMENTATION WORKFLOW
================================================================================

TRAINING PHASE:
    1. Initialize Q-table with random small values
    2. Set ε = 1.0 (full exploration)
    3. For each episode:
        a. Reset environment (random angle)
        b. While not terminated:
            - Get discrete state
            - Choose action (ε-greedy)
            - Execute action
            - Observe reward and next state
            - Update Q(s,a) using Q-learning rule
            - Increment step counter
        c. Decay ε after episode
        d. Track performance metrics
    4. Save Q-table when satisfied

TESTING PHASE:
    1. Load trained Q-table
    2. Set ε = 0 (pure exploitation)
    3. Watch agent balance pole
    4. No Q-table updates

TYPICAL TRAINING:
    - Episodes needed: 500-2000
    - Early episodes: Fails quickly (< 20 steps)
    - Mid training: Occasional success (50-200 steps)
    - Well-trained: Consistent balancing (500+ steps)

================================================================================
6. KEY DESIGN DECISIONS
================================================================================

6.1 State Discretization
------------------------
Why discretize?
    - Q-table requires finite states
    - Continuous space would need function approximation (DQN)

Trade-off:
    + Simpler implementation
    + Faster learning for small problems
    - Limited resolution
    - Curse of dimensionality

Current: 9,216 states (manageable)

6.2 Action Space
----------------
11 discrete force levels instead of continuous:
    + Reduces action space complexity
    + Finer control near zero (where precision matters)
    + Coarser at extremes (less critical)
    - Not truly continuous

6.3 Reward Shaping
------------------
Dense rewards vs sparse:
    Sparse: +1 per step, -1 on fall (simple but slow learning)
    Dense: Shaped reward based on angle, velocity, position
    
    Current approach:
        ✓ Provides learning signal every step
        ✓ Guides toward desired behavior
        ✓ Faster convergence
        - Requires domain knowledge

6.4 Physics Timestep
--------------------
dt = 0.02s (50 Hz):
    + Fast enough for smooth simulation
    + Slow enough for stable integration
    + Matches typical control rates

================================================================================
7. PHYSICS EQUATIONS (SIMPLIFIED)
================================================================================

State Vector:
    z = [x, θ, x_dot, θ_dot]ᵀ

Equations of Motion:
    θ̈ (angular acceleration) = f₁(x, θ, ẋ, θ̇, u)
    ẍ (cart acceleration) = f₂(x, θ, ẋ, θ̇, u)

Derived from:
    - Lagrangian mechanics
    - Euler-Lagrange equations
    - Includes coupling between cart and pole motion

Integration:
    z(t+Δt) = z(t) + Δt × ż(t)  [Euler method]

================================================================================
8. PERFORMANCE METRICS
================================================================================

Success Indicators:
    - Episode length > 200 steps: Good
    - Episode length > 500 steps: Very good
    - Consistent 500+ over 100 episodes: Excellent
    - Average plateaus: Converged policy

Monitoring:
    - Best episode: Peak performance
    - Average (100 episodes): Consistency measure
    - Epsilon: Exploration level
    - Steps per episode: Learning progress

Typical Learning Curve:
    Episodes 0-100:    Rapid failure (< 30 steps)
    Episodes 100-500:  Improvement (30-200 steps)
    Episodes 500-1000: Mastery (200+ steps)
    Episodes 1000+:    Maintenance (consistent performance)

================================================================================
9. ADVANTAGES OF THIS IMPLEMENTATION
================================================================================

Educational:
    ✓ Clear separation of concerns (physics, agent, visualization)
    ✓ Easy to understand Q-learning implementation
    ✓ Visual feedback for learning process
    ✓ Interactive controls for experimentation

Practical:
    ✓ Fast training (minutes, not hours)
    ✓ Deterministic physics (reproducible)
    ✓ Save/load functionality
    ✓ Real-time visualization

Extensible:
    ✓ Easy to modify reward function
    ✓ Can adjust discretization resolution
    ✓ Can add noise or stochasticity
    ✓ Foundation for Deep Q-Networks (DQN)

================================================================================
10. LIMITATIONS AND FUTURE EXTENSIONS
================================================================================

Current Limitations:
    - Discrete states (resolution limit)
    - Tabular Q-learning (doesn't scale)
    - No generalization between states
    - 4D state space (higher dimensions intractable)

Potential Extensions:
    1. Deep Q-Network (DQN):
       - Neural network approximates Q(s,a)
       - Handles continuous states
       - Better scalability

    2. Double DQN / Dueling DQN:
       - Reduces overestimation
       - Separates value and advantage

    3. Policy Gradient Methods:
       - REINFORCE, Actor-Critic, PPO
       - Direct policy learning

    4. Add Noise/Uncertainty:
       - Wind disturbances
       - Sensor noise
       - Actuator delays

    5. Multi-Pole Balancing:
       - Multiple poles on cart
       - More complex dynamics

    6. Transfer Learning:
       - Train on one configuration
       - Test on different physics

================================================================================
11. USAGE GUIDE
================================================================================

Starting the Program:
    python main.py

Initial Behavior:
    - Agent starts in training mode
    - Pole will fall quickly at first
    - Performance improves over episodes

Training:
    1. Let it run for 500-1000 episodes
    2. Monitor average score increasing
    3. Press 'S' to save when satisfied
    4. Press 'F' for fast training (no rendering)

Testing:
    1. Press 'L' to load saved Q-table
    2. Press 'T' to disable training
    3. Watch agent balance pole
    4. Should balance indefinitely

Troubleshooting:
    - Not learning: Check reward function, α, γ
    - Unstable: Reduce learning rate α
    - Converged poorly: Train longer, adjust ε decay
    - Reset: Press 'R' to start fresh

================================================================================
12. TECHNICAL IMPLEMENTATION NOTES
================================================================================

Dependencies:
    - numpy: Numerical computations
    - pygame: Real-time visualization and control
    - pickle: Q-table serialization

Design Patterns:
    - Object-oriented environment (CartPole class)
    - Separate agent class (decoupled from environment)
    - Configuration file for easy tuning
    - Event-driven controls (pygame events)

Performance:
    - Runs at 50 Hz (real-time)
    - Fast mode skips rendering (100x faster)
    - Q-table lookup: O(1)
    - Update: O(1)

Memory:
    - Q-table: ~800 KB (101,376 floats × 8 bytes)
    - Efficient for this problem size
    - Larger discretizations may require DQN

================================================================================
                                   END
================================================================================

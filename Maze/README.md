# ğŸ§© Maze Reinforcement Learning Project

Grid-based maze environment implementing classic **Reinforcement Learning (RL)** algorithms with rich **interactive visualization** using Matplotlib.

---

## ğŸ“Œ Project Overview

* **Environment**: Grid-based Maze (MDP)
* **Algorithms**:

  * Value Iteration
  * Policy Iteration
  * Greedy Policies
* **Visualization**:

  * Interactive Matplotlib board
  * State values, Q-values, policy arrows
  * Animated agent simulation

---

## ğŸ“ Project Structure

```
Maze/
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ maze.py         # Main RL algorithms and environment logic
    â”œâ”€â”€ board.py        # MazeBoard class for visualization and rendering
    â”œâ”€â”€ cells.py        # Cell types and state representation
    â””â”€â”€ __pycache__/    # Python bytecode cache
```

---

## ğŸ§  Core Components

### 1ï¸âƒ£ `cells.py` â€“ State Representation

Defines the maze cell hierarchy and state abstraction.

#### ğŸ”¹ Position

* Immutable `@dataclass`
* Attributes: `row`, `col`
* Callable â†’ returns `(row, col)`

#### ğŸ”¹ Actions (Enum)

```text
UP = 0
DOWN = 1
LEFT = 2
RIGHT = 3
```

#### ğŸ”¹ Cell (Abstract Base Class)

**Properties**:

* `reward: float`
* `steppable: bool`
* `terminal: bool`
* `teleport: bool`

**Methods**:

* `get_reward()`
* `is_steppable()`
* `is_terminal()`
* `is_teleport()`
* `get_next_cell()`

#### ğŸ”¹ Cell Types

| Cell Type    | Description                                                   |
| ------------ | ------------------------------------------------------------- |
| **RegCell**  | Regular cell (`-1`) or penalty cell (`-10`), color: white/red |
| **TermCell** | Terminal (goal) cell, reward `0`, color: yellow               |
| **WallCell** | Obstacle, non-steppable, color: black                         |
| **TelCell**  | Teleport cell with destination position, color: blue          |

---

### 2ï¸âƒ£ `board.py` â€“ Environment Visualization

Handles maze rendering and interaction via **Matplotlib**.

#### ğŸ”¹ Initialization

* Random grid generation:

  * 67% regular cells (`-1`)
  * 13% penalty cells (`-10`)
  * 13% walls
  * 7% teleports
* One randomly placed terminal cell
* Mouse click event handling

#### ğŸ”¹ Visualization Features

* Color-coded grid
* State values `V(s)`
* Policy arrows (â†‘ â†“ â† â†’)
* Full Q-value display per cell
* Animated agent movement
* Teleport index mapping

#### ğŸ”¹ Main Methods

* `draw_board()`
* `draw_values(values)`
* `draw_actions(policy)`
* `draw_q_values(q_values)`
* `draw_agent(position, symbol)`
* `onclick(event)`

---

### 3ï¸âƒ£ `maze.py` â€“ RL Algorithms

Contains the **MazeEnvironment** and core RL logic.

#### ğŸ”¹ MazeEnvironment

* Wraps `MazeBoard`
* Extracts valid (steppable) states
* Handles teleports and rewards
* Defines state transitions

**Key Methods**:

* `init_states()`
* `init_actions()`
* `next_state(s, a)`
* `update_state_value()`
* `update_all_state_values()`
* `update_action_value()`
* `update_all_action_values()`

---

## ğŸ¤– Implemented Algorithms

### A) Value Iteration

* Uses Bellman optimality equation:

```
V(s) = maxâ‚ [ R(s,a) + Î³ Â· V(s') ]
```

* Iterates until convergence (`error < Îµ`)
* Produces optimal value function **V***

---

### B) Policy Iteration

1. **Policy Evaluation** â†’ Compute `V^Ï€(s)`
2. **Policy Improvement** â†’ Update `Ï€(s)` via greedy Q

* Repeats until policy stabilizes
* Often faster than Value Iteration

---

### C) Greedy Policies

* `greedy()` â†’ based on `V(s)`
* `greedy_q()` â†’ based on `Q(s,a)`

Used for policy improvement and execution.

---

### D) Policy Execution

* `apply_policy()` simulates one episode
* Returns total discounted reward (gain)
* Animates agent movement on the board

---

## ğŸ” Basic Workflow

### Step 1: Environment Setup

```python
board = MazeBoard(rows=10, cols=10)
env = MazeEnvironment(board)
```

### Step 2: Initialize Values & Policy

```python
v = {s: 0 for s in env.states}
policy = {s: random.choice(list(Actions)).name for s in env.states}
```

### Step 3: Value Iteration

```python
v_optimal = value_iteration(
    update=env.update_all_state_values,
    values=v,
    gamma=0.9,
    eps=0.01,
    iterations=100
)
```

### Step 4: Extract Greedy Policy

```python
optimal_policy = {
    s: greedy(env, s, v_optimal, gamma=0.9).name
    for s in env.states
}
```

### Step 5: Visualization & Simulation

```python
board.draw_board()
board.draw_values(v_optimal)
board.draw_actions(optimal_policy)

gain = apply_policy(env, greedy, start_state, gamma, v_optimal)
```

---

## ğŸ“˜ Reinforcement Learning Concepts

### ğŸ”¹ Markov Decision Process (MDP)

* **States**: Grid positions
* **Actions**: {UP, DOWN, LEFT, RIGHT}
* **Transitions**: Deterministic
* **Rewards**: Cell-based
* **Î³ (Discount factor)**

---

### ğŸ”¹ Bellman Equations

**Optimality (VI)**:

```
V*(s) = maxâ‚ [ R(s,a) + Î³ Â· V*(s') ]
```

**Policy Evaluation (PI)**:

```
V^Ï€(s) = Î£ Ï€(a|s) [ R(s,a) + Î³ Â· V^Ï€(s') ]
```

---

### ğŸ”¹ Convergence

* Tracks max value change
* Stops when:

```
max |V_new - V_old| < Îµ
```

* Guaranteed for `Î³ < 1`

---

## âœ¨ Special Features

### ğŸ”¹ Teleportation

* Teleport cells redirect agent instantly
* Reward equals destination cell reward
* Invalid teleports auto-corrected
* Visualized with blue cells + index labels

### ğŸ”¹ Interactive UI

* Mouse click inspection
* Live Q-value rendering
* Smooth agent animation
* Clear color-coded grid

---

## âš™ï¸ Key Parameters

### Discount Factor (Î³)

* `0.9 â€“ 0.99`: long-term planning
* `0.1 â€“ 0.5`: short-term rewards

### Convergence Threshold (Îµ)

* Smaller â†’ more precise, slower
* Larger â†’ faster, less accurate

### Iterations

* Typical range: `50 â€“ 200`
* Safety cap for convergence


